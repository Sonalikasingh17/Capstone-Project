# ğŸš€ Capstone Project â€“ End-to-End MLOps Pipeline

![Python](https://img.shields.io/badge/Python-3.10-blue?logo=python)
![MLflow](https://img.shields.io/badge/MLflow-Tracking-orange?logo=mlflow)
![AWS](https://img.shields.io/badge/AWS-EKS%20|%20S3%20|%20ECR-orange?logo=amazonaws)
![Docker](https://img.shields.io/badge/Docker-Containerization-blue?logo=docker)
![GitHub Actions](https://img.shields.io/badge/CI/CD-GitHub%20Actions-2088FF?logo=githubactions)
![Prometheus](https://img.shields.io/badge/Prometheus-Monitoring-red?logo=prometheus)
![Grafana](https://img.shields.io/badge/Grafana-Dashboard-orange?logo=grafana)

> ğŸ§  A complete **Machine Learning Operations (MLOps)** pipeline â€” from local development to cloud deployment and monitoring.

---

## ğŸ§© Project Overview

This Capstone Project automates the **end-to-end ML lifecycle** using modern DevOps and MLOps tools.  
Youâ€™ll learn how to build, version, deploy, and monitor a machine learning model in production ğŸŒâš™ï¸

**Key Features:**
- ğŸ§® Data pipeline: Ingestion â†’ Preprocessing â†’ Feature Engineering  
- ğŸ¤– Model lifecycle: Training â†’ Evaluation â†’ Registration  
- ğŸ“Š Experiment tracking using **MLflow (Dagshub)**  
- ğŸ’¾ Dataset versioning using **DVC + AWS S3**  
- ğŸ³ Containerization using **Docker**  
- ğŸ” Deployment automation using **GitHub Actions + AWS ECR + EKS**  
- ğŸ“ˆ Monitoring using **Prometheus + Grafana**

---

## ğŸ—ï¸ Project Structure


Capstone-Project/
```
â”‚
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ logger/
â”‚ â”œâ”€â”€ data_ingestion.py
â”‚ â”œâ”€â”€ data_preprocessing.py
â”‚ â”œâ”€â”€ feature_engineering.py
â”‚ â”œâ”€â”€ model_building.py
â”‚ â”œâ”€â”€ model_evaluation.py
â”‚ â”œâ”€â”€ register_model.py
â”‚
â”œâ”€â”€ flask_app/
â”‚ â”œâ”€â”€ app.py
â”‚ â”œâ”€â”€ templates/
â”‚ â”œâ”€â”€ static/
â”‚
â”œâ”€â”€ tests/
â”œâ”€â”€ scripts/
â”œâ”€â”€ params.yaml
â”œâ”€â”€ dvc.yaml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ .github/workflows/ci.yaml
â””â”€â”€ README.md
```
---

## âš™ï¸ Tech Stack

| Category | Tools |
|-----------|--------|
| ğŸ§  **ML Frameworks** | Scikit-learn, Pandas, NumPy |
| ğŸ’¾ **Versioning & Tracking** | DVC, MLflow, Dagshub |
| ğŸ³ **Containerization** | Docker |
| â˜ï¸ **Cloud Services** | AWS S3, ECR, EKS |
| ğŸ” **Automation** | GitHub Actions |
| ğŸ“Š **Monitoring** | Prometheus, Grafana |
| ğŸ§° **Environment** | Conda, Python 3.10 |

---

</details>
<details> <summary>ğŸ§° <b>1ï¸âƒ£ Project Setup</b></summary>

```bash
# Create virtual environment
conda create -n atlas python=3.10
conda activate atlas

# Install template
pip install cookiecutter
cookiecutter -c v1 https://github.com/drivendata/cookiecutter-data-science

# Rename and initialize repo
mv src.models src.model
git add .
git commit -m "Initial setup"
git push
```

</details> 

<details> <summary>ğŸ“Š <b>2ï¸âƒ£ Setup MLflow Tracking (Dagshub)</b></summary>

- Go to Dagshub Dashboard
- Create a new repo and connect your GitHub repository
- Copy the MLflow tracking URL & integrate it into your notebooks
- Install dependencies

```bash
pip install dagshub mlflow
```
</details> 

<details> <summary>ğŸ—ƒï¸ <b>3ï¸âƒ£ Setup Data Version Control (DVC)</b></summary>
  
```bash
dvc init
mkdir local_s3
dvc remote add -d mylocal local_s3
```

Add these files:
- dvc.yaml
- params.yaml

Then run:

```bash
dvc repro
dvc status
git add .
git commit -m "DVC pipeline ready"
git push
```

</details>
<details> <summary>â˜ï¸ <b>4ï¸âƒ£ Connect AWS S3 Storage</b></summary>
  
```bash
pip install dvc[s3] awscli
aws configure
dvc remote add -d myremote s3://<your-bucket-name>
dvc push
```

</details>
<details> <summary>ğŸŒ <b>5ï¸âƒ£ Flask App Setup</b></summary>

```bash
cd flask_app
pip install flask
python app.py
```

Your local app will be available at

```cpp
ğŸ‘‰ http://127.0.0.1:5000
```

</details>
<details> <summary>ğŸ¤– <b>6ï¸âƒ£ CI/CD Pipeline Setup</b></summary>

- Add .github/workflows/ci.yaml
- Generate a token from Dagshub â†’ Settings â†’ Tokens
- Save token in GitHub â†’ Secrets â†’ CAPSTONE_TEST
- Add testing directories:

```bash
tests/
scripts/
```

</details>
<details> <summary>ğŸ³ <b>7ï¸âƒ£ Dockerize the App</b></summary>

```bash
cd flask_app
pip install pipreqs
pipreqs . --force
cd ..
docker build -t capstone-app:latest .
docker run -p 8888:5000 -e CAPSTONE_TEST=<your_token> capstone-app:latest
```

</details>
<details> <summary>â˜ï¸ <b>8ï¸âƒ£ AWS EKS Deployment</b></summary>

```bash
eksctl create cluster --name flask-app-cluster --region us-east-1 \
--nodegroup-name flask-app-nodes --node-type t3.small \
--nodes 1 --nodes-min 1 --nodes-max 1 --managed
```

Then check:

```bash
aws eks list-clusters
kubectl get nodes
kubectl get svc flask-app-service
```

Visit your deployed app via:

```cpp
http://<external-ip>:5000
```

</details>
<details> <summary>ğŸ“ˆ <b>9ï¸âƒ£ Monitoring with Prometheus & Grafana</b></summary>
  
ğŸ§­ Prometheus (Port 9090)

```bash
wget https://github.com/prometheus/prometheus/releases/download/v2.46.0/prometheus-2.46.0.linux-amd64.tar.gz
tar -xvzf prometheus-2.46.0.linux-amd64.tar.gz
sudo mv prometheus /etc/prometheus
sudo mv /etc/prometheus/prometheus /usr/local/bin/
```

Update config file:

```yaml
scrape_configs:
  - job_name: "flask-app"
    static_configs:
      - targets: ["<external-ip>:5000"]
```

Run:

```bash
/usr/local/bin/prometheus --config.file=/etc/prometheus/prometheus.yml
```

ğŸ“Š Grafana (Port 3000)

```bash
wget https://dl.grafana.com/oss/release/grafana_10.1.5_amd64.deb
sudo apt install ./grafana_10.1.5_amd64.deb -y
sudo systemctl start grafana-server
sudo systemctl enable grafana-server
```

Access at:

```cpp
ğŸ‘‰ http://<ec2-public-ip>:3000 (admin / admin)
```
</details>


---


## ğŸ§¹ AWS Resource Cleanup

Clean up resources after testing to avoid extra AWS costs ğŸ’¸

```bash
# Delete Kubernetes deployment and service
kubectl delete deployment flask-app
kubectl delete service flask-app-service
kubectl delete secret capstone-secret

# Delete EKS Cluster
eksctl delete cluster --name flask-app-cluster --region us-east-1

# Verify deletion
eksctl get cluster --region us-east-1

```
Also:
- ğŸ§¹ Delete ECR images
- ğŸ§º Remove S3 buckets
- ğŸ“¦ Check CloudFormation and delete remaining stacks if needed

---


## ğŸ§© Key Concepts

| Concept              | Description                            |
| -------------------- | -------------------------------------- |
| **MLflow**           | Track experiments, metrics, and models |
| **DVC**              | Data & model version control           |
| **Docker**           | Package and run app in containers      |
| **EKS (Kubernetes)** | Deploy containerized app on AWS        |
| **Prometheus**       | Collect and store metrics              |
| **Grafana**          | Visualize app health dashboards        |

---

## ğŸ Final Results

- âœ… Fully automated end-to-end ML pipeline
- âœ… Deployed on AWS with Docker & EKS
- âœ… CI/CD integration via GitHub Actions
- âœ… Real-time monitoring using Prometheus & Grafana

---

## ğŸ‘©â€ğŸ’» Author
**Sonalika Singh**
- ğŸ“«[Medium Blogs](https://medium.com/@singhsonalika5)
- ğŸŒ [GitHub Profile](https://github.com/Sonalikasingh17)

---

## ğŸŒŸ Star This Repository

If you find this project helpful, please give it a â­ on [GitHub](https://github.com/Sonalikasingh17/Capstone-Project)
 â€” it keeps me motivated! ğŸ’ª

---

## ğŸ§­ Next Steps

 - Add unit testing for each pipeline stage
 - Integrate Kubernetes auto-scaling
 - Enhance Grafana dashboard visualizations

 ---

â€œBuild once, automate forever.â€ ğŸ’¡

â€” *Sonalika Singh*
